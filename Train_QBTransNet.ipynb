{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530b2877-0595-48a2-8e17-c6c023dfad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D,Flatten,MaxPooling2D,ZeroPadding2D,Concatenate,Lambda,Softmax,GlobalAveragePooling1D,MaxPooling1D,SpatialDropout1D,ReLU, Dense, Activation,Reshape,BatchNormalization, add, Embedding,Conv1D,LayerNormalization,MultiHeadAttention,Add,Dropout,Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from Uilts import *\n",
    "from capsulelayers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254aa444-797c-4e26-827c-3189148b032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datall shape: (6444, 20, 6000, 3)\n",
      "laballx shape: (6444,)\n",
      "Earthquakes: 3169 Quarry blasts: 3275\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ========== Load dataset from HDF5 file ==========\n",
    "# This file contains two datasets:\n",
    "# - 'datall': the input data (e.g., scalograms)\n",
    "# - 'laballx': the corresponding labels (0 for EQ, 1 for QB)\n",
    "\n",
    "with h5py.File('./Dataset/data_train.h5', 'r') as hf:\n",
    "    datall = hf['datall'][:]     # Load all input data into memory\n",
    "    laballx = hf['laballx'][:]   # Load all labels into memory\n",
    "\n",
    "# ========== Print dataset shapes ==========\n",
    "print(\"datall shape:\", datall.shape)   \n",
    "print(\"laballx shape:\", laballx.shape) \n",
    "\n",
    "# ========== Convert integer labels to one-hot encoding ==========\n",
    "# Converts labels:\n",
    "#   0 → [1, 0]  (Earthquake)\n",
    "#   1 → [0, 1]  (Quarry Blast)\n",
    "laball = tf.keras.utils.to_categorical(laballx, num_classes=2)\n",
    "\n",
    "# ========== Print label distribution ==========\n",
    "print('Earthquakes:',len(np.where(laballx == 0)[0]), 'Quarry blasts:',len(np.where(laballx == 1)[0]))  # Count EQ and QB samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c414d5fa-50b1-417f-89ad-52c310e3ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5799, 20, 6000, 3)\n",
      "(5799, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# np.random.seed(42)\n",
    "# io = np.random.permutation(len(datall))\n",
    "# datall = datall[io]\n",
    "# laball = laball[io]\n",
    "\n",
    "sp = 0.1\n",
    "x_train, x_test, y_train, y_test = train_test_split(datall, laball, test_size=sp, random_state=2024)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e1859d-db10-487f-ba6d-d07a64c273c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the network\n",
    "\n",
    "w1 = 20\n",
    "w2 = 6000\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "stochastic_depth_rate = 0.1\n",
    "positional_emb = False\n",
    "conv_layers = 6\n",
    "num_classes = 1\n",
    "input_shape = (w1, w2, 3)\n",
    "image_size = w1  # We'll resize input images to this size\n",
    "projection_dim = int(w1)\n",
    "num_heads = 3\n",
    "transformer_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]  # Number of the transformer layers\n",
    "transformer_layers = 1\n",
    "mlp_head_units = [\n",
    "    1024,\n",
    "    512,\n",
    "]\n",
    "\n",
    "image_size = 6000\n",
    "patch_size = 20\n",
    "num_patches = (image_size // patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8355c84-bb6c-4a9e-a84f-741c7b6aeb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Transformer-based Feature Extractor =========\n",
    "# Required: Custom components such as DCBlock, Patches, PatchEncoder, mlp, StochasticDepth\n",
    "def create_cct_model(inputs):\n",
    "    \"\"\"\n",
    "    Feature extractor using Dilated Convolution Blocks and Transformer layers.\n",
    "    \"\"\"\n",
    "    # Apply multiple dilated convolutional blocks\n",
    "    x = DCBlock(inputs, 6, (3, 3), 0.1)\n",
    "    x = DCBlock(x, 12, (3, 3), 0.1)\n",
    "    x = DCBlock(x, 20, (3, 3), 0.1)\n",
    "\n",
    "    # Patch embedding\n",
    "    patches = Patches(patch_size)(x)\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Define stochastic depth rates for each Transformer layer\n",
    "    dpr = np.linspace(0, stochastic_depth_rate, transformer_layers)\n",
    "\n",
    "    # Stack multiple Transformer blocks\n",
    "    for i in range(transformer_layers):\n",
    "        # Layer normalization\n",
    "        x1 = LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "\n",
    "        # Multi-head self-attention\n",
    "        attn_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection and stochastic depth\n",
    "        attn_output = StochasticDepth(dpr[i])(attn_output)\n",
    "        x2 = Add()([attn_output, encoded_patches])\n",
    "\n",
    "        # Second layer normalization\n",
    "        x3 = LayerNormalization(epsilon=1e-5)(x2)\n",
    "\n",
    "        # Feed-forward MLP with dropout\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.2)\n",
    "\n",
    "        # Skip connection and stochastic depth\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    # Final layer normalization\n",
    "    representation = LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    return representation\n",
    "\n",
    "\n",
    "# ========= Capsule Network Model =========\n",
    "\n",
    "def QBTransNet(input_shape, n_class, routings):\n",
    "    \"\"\"\n",
    "    Capsule Network with Transformer-based encoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_shape: shape of input data [height, width, channels]\n",
    "    - n_class: number of target classes\n",
    "    - routings: number of dynamic routing iterations\n",
    "    \n",
    "    Returns:\n",
    "    - Compiled Keras model\n",
    "    \"\"\"\n",
    "    capsule_dim = 4\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Transformer feature extractor\n",
    "    conv_features = create_cct_model(inputs)\n",
    "\n",
    "    # Reshape encoded sequence into spatial format\n",
    "    conv_features = Reshape((20, 15, 20))(conv_features)\n",
    "\n",
    "    # Primary capsule layer: extracts low-level capsules\n",
    "    primarycaps = PrimaryCap(\n",
    "        conv_features, dim_capsule=capsule_dim, n_channels=4,\n",
    "        kernel_size=5, strides=(1, 4), padding='same'\n",
    "    )\n",
    "    primarycaps = Dropout(0.5)(primarycaps)\n",
    "\n",
    "    # Digit capsule layer: high-level capsules with dynamic routing\n",
    "    digitcaps = CapsuleLayer(\n",
    "        num_capsule=n_class, dim_capsule=capsule_dim,\n",
    "        routings=routings, name='digitcaps'\n",
    "    )(primarycaps)\n",
    "\n",
    "    # Output capsule length as probability\n",
    "    out_caps = Length(name='capsnet')(digitcaps)\n",
    "\n",
    "    # Build and compile model\n",
    "    model = Model(inputs=inputs, outputs=out_caps)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                  loss=[margin_loss], metrics=['acc'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def run_experiment(model):\n",
    "    \"\"\"\n",
    "    Train the provided model with training data and specified callbacks.\n",
    "\n",
    "    Returns:\n",
    "        history: Training history object containing loss and accuracy metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Callback to reduce learning rate when validation accuracy plateaus\n",
    "    lr_reducer = ReduceLROnPlateau(\n",
    "        factor=np.sqrt(0.1),       # Reduce LR by sqrt(0.1)\n",
    "        cooldown=0,                # No cooldown period before resuming LR adjustments\n",
    "        patience=20,               # Wait 20 epochs before reducing LR\n",
    "        min_lr=0.5e-8,             # Minimum allowable LR\n",
    "        monitor='val_acc',         # Monitor validation accuracy\n",
    "        mode='max'                 # Looking for maximum accuracy\n",
    "    )\n",
    "\n",
    "    # Callback to stop training early if validation accuracy doesn't improve\n",
    "    early_stopping_monitor = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        mode='max',\n",
    "        patience=30                # Stop training after 30 epochs of no improvement\n",
    "    )\n",
    "\n",
    "    # Callback to save the best model weights during training\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath='Epochs/best_model.h5',\n",
    "        monitor='val_acc',        # Monitor validation accuracy\n",
    "        mode='max',\n",
    "        save_best_only=True,      # Save only the best model\n",
    "        verbose=1                 # Print updates when model is saved\n",
    "    )\n",
    "\n",
    "    # Combine all callbacks into a list\n",
    "    callbacks = [lr_reducer, early_stopping_monitor, checkpoint]\n",
    "\n",
    "    # Train the model with training data and 10% validation split\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=200,\n",
    "        validation_split=0.1,\n",
    "        batch_size=16,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2c8a66-e596-4f75-a216-99db2ad1fdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\liuya444\\AppData\\Local\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py:7101: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 6000, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 20, 6000, 3)  84          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 20, 6000, 3)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 20, 6000, 3)  84          ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 20, 6000, 3)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 20, 6000, 3)  84          ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 20, 6000, 3)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 20, 6000, 3)  0           ['activation_2[0][0]',           \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 20, 6000, 6)  168         ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 20, 6000, 6)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20, 6000, 6)  0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 20, 6000, 6)  330         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 20, 6000, 6)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 20, 6000, 6)  330         ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 20, 6000, 6)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 20, 6000, 6)  330         ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 20, 6000, 6)  0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 20, 6000, 6)  0           ['activation_6[0][0]',           \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 20, 6000, 12  660         ['add_1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 20, 6000, 12  0           ['conv2d_7[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 20, 6000, 12  0           ['activation_7[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 20, 6000, 12  1308        ['dropout_1[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 20, 6000, 12  0           ['conv2d_8[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 20, 6000, 12  1308        ['activation_8[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 20, 6000, 12  0           ['conv2d_9[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 20, 6000, 12  1308        ['activation_9[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 20, 6000, 12  0           ['conv2d_10[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 20, 6000, 12  0           ['activation_10[0][0]',          \n",
      "                                )                                 'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 20, 6000, 20  2180        ['add_2[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 20, 6000, 20  0           ['conv2d_11[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 20, 6000, 20  0           ['activation_11[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " patches (Patches)              (None, None, 8000)   0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 300, 20)      166020      ['patches[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 300, 20)     40          ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 300, 20)     5000        ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " stochastic_depth (StochasticDe  (None, 300, 20)     0           ['multi_head_attention[0][0]']   \n",
      " pth)                                                                                             \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 300, 20)      0           ['stochastic_depth[0][0]',       \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 300, 20)     40          ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 300, 20)      420         ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 300, 20)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 300, 20)      420         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 300, 20)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " stochastic_depth_1 (Stochastic  (None, 300, 20)     0           ['dropout_4[0][0]']              \n",
      " Depth)                                                                                           \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 300, 20)      0           ['stochastic_depth_1[0][0]',     \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 300, 20)     40          ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 20, 15, 20)   0           ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " primarycap_conv2d (Conv2D)     (None, 20, 4, 16)    8016        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " primarycap_reshape (Reshape)   (None, 320, 4)       0           ['primarycap_conv2d[0][0]']      \n",
      "                                                                                                  \n",
      " primarycap_squash (Lambda)     (None, 320, 4)       0           ['primarycap_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 320, 4)       0           ['primarycap_squash[0][0]']      \n",
      "                                                                                                  \n",
      " digitcaps (CapsuleLayer)       (None, 2, 4)         10240       ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " capsnet (Length)               (None, 2)            0           ['digitcaps[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 198,410\n",
      "Trainable params: 198,410\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.1047 - acc: 0.8582\n",
      "Epoch 1: val_acc improved from -inf to 0.92586, saving model to Epochs\\best_model.h5\n",
      "327/327 [==============================] - 29s 78ms/step - loss: 0.1047 - acc: 0.8582 - val_loss: 0.0554 - val_acc: 0.9259 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9224\n",
      "Epoch 2: val_acc improved from 0.92586 to 0.93276, saving model to Epochs\\best_model.h5\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0607 - acc: 0.9224 - val_loss: 0.0464 - val_acc: 0.9328 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9473\n",
      "Epoch 3: val_acc improved from 0.93276 to 0.96207, saving model to Epochs\\best_model.h5\n",
      "327/327 [==============================] - 25s 76ms/step - loss: 0.0410 - acc: 0.9473 - val_loss: 0.0263 - val_acc: 0.9621 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9601\n",
      "Epoch 4: val_acc improved from 0.96207 to 0.97069, saving model to Epochs\\best_model.h5\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0333 - acc: 0.9601 - val_loss: 0.0222 - val_acc: 0.9707 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9705\n",
      "Epoch 5: val_acc did not improve from 0.97069\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0263 - acc: 0.9705 - val_loss: 0.0198 - val_acc: 0.9707 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9737\n",
      "Epoch 6: val_acc did not improve from 0.97069\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0232 - acc: 0.9737 - val_loss: 0.0271 - val_acc: 0.9603 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9797\n",
      "Epoch 7: val_acc did not improve from 0.97069\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0195 - acc: 0.9797 - val_loss: 0.0200 - val_acc: 0.9690 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9783\n",
      "Epoch 8: val_acc did not improve from 0.97069\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0192 - acc: 0.9783 - val_loss: 0.0188 - val_acc: 0.9690 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9864\n",
      "Epoch 9: val_acc improved from 0.97069 to 0.97586, saving model to Epochs\\best_model.h5\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0135 - acc: 0.9864 - val_loss: 0.0166 - val_acc: 0.9759 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9852\n",
      "Epoch 10: val_acc did not improve from 0.97586\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0137 - acc: 0.9852 - val_loss: 0.0188 - val_acc: 0.9759 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9795\n",
      "Epoch 11: val_acc improved from 0.97586 to 0.98276, saving model to Epochs\\best_model.h5\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0183 - acc: 0.9795 - val_loss: 0.0138 - val_acc: 0.9828 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9847\n",
      "Epoch 12: val_acc did not improve from 0.98276\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0131 - acc: 0.9847 - val_loss: 0.0203 - val_acc: 0.9690 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9850\n",
      "Epoch 13: val_acc did not improve from 0.98276\n",
      "327/327 [==============================] - 25s 76ms/step - loss: 0.0137 - acc: 0.9851 - val_loss: 0.0153 - val_acc: 0.9776 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9860\n",
      "Epoch 14: val_acc did not improve from 0.98276\n",
      "327/327 [==============================] - 25s 76ms/step - loss: 0.0141 - acc: 0.9860 - val_loss: 0.0137 - val_acc: 0.9828 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9914\n",
      "Epoch 15: val_acc did not improve from 0.98276\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0091 - acc: 0.9914 - val_loss: 0.0150 - val_acc: 0.9776 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9893\n",
      "Epoch 16: val_acc did not improve from 0.98276\n",
      "327/327 [==============================] - 25s 76ms/step - loss: 0.0115 - acc: 0.9893 - val_loss: 0.0150 - val_acc: 0.9793 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9906\n",
      "Epoch 17: val_acc did not improve from 0.98276\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0103 - acc: 0.9906 - val_loss: 0.0119 - val_acc: 0.9828 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9948\n",
      "Epoch 18: val_acc improved from 0.98276 to 0.98621, saving model to Epochs\\best_model.h5\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0068 - acc: 0.9948 - val_loss: 0.0118 - val_acc: 0.9862 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9944\n",
      "Epoch 19: val_acc improved from 0.98621 to 0.98793, saving model to Epochs\\best_model.h5\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0066 - acc: 0.9944 - val_loss: 0.0120 - val_acc: 0.9879 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9937\n",
      "Epoch 20: val_acc did not improve from 0.98793\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0087 - acc: 0.9937 - val_loss: 0.0124 - val_acc: 0.9862 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9956\n",
      "Epoch 21: val_acc did not improve from 0.98793\n",
      "327/327 [==============================] - 24s 74ms/step - loss: 0.0058 - acc: 0.9956 - val_loss: 0.0137 - val_acc: 0.9793 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9937\n",
      "Epoch 22: val_acc did not improve from 0.98793\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0074 - acc: 0.9937 - val_loss: 0.0126 - val_acc: 0.9845 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9964\n",
      "Epoch 23: val_acc did not improve from 0.98793\n",
      "327/327 [==============================] - 24s 74ms/step - loss: 0.0048 - acc: 0.9964 - val_loss: 0.0120 - val_acc: 0.9759 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9948\n",
      "Epoch 24: val_acc improved from 0.98793 to 0.98966, saving model to Epochs\\best_model.h5\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0060 - acc: 0.9948 - val_loss: 0.0099 - val_acc: 0.9897 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9941\n",
      "Epoch 25: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0080 - acc: 0.9941 - val_loss: 0.0149 - val_acc: 0.9793 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9962\n",
      "Epoch 26: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0058 - acc: 0.9962 - val_loss: 0.0167 - val_acc: 0.9793 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9941\n",
      "Epoch 27: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 76ms/step - loss: 0.0068 - acc: 0.9941 - val_loss: 0.0132 - val_acc: 0.9828 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9969\n",
      "Epoch 28: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 76ms/step - loss: 0.0050 - acc: 0.9969 - val_loss: 0.0278 - val_acc: 0.9655 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9954\n",
      "Epoch 29: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0057 - acc: 0.9954 - val_loss: 0.0121 - val_acc: 0.9845 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9979\n",
      "Epoch 30: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0037 - acc: 0.9979 - val_loss: 0.0091 - val_acc: 0.9897 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9965\n",
      "Epoch 31: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0050 - acc: 0.9966 - val_loss: 0.0127 - val_acc: 0.9828 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9981\n",
      "Epoch 32: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 76ms/step - loss: 0.0032 - acc: 0.9981 - val_loss: 0.0111 - val_acc: 0.9828 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9910\n",
      "Epoch 33: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0089 - acc: 0.9910 - val_loss: 0.0120 - val_acc: 0.9828 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9954\n",
      "Epoch 34: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0050 - acc: 0.9954 - val_loss: 0.0111 - val_acc: 0.9845 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9965\n",
      "Epoch 35: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0049 - acc: 0.9966 - val_loss: 0.0113 - val_acc: 0.9810 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9969\n",
      "Epoch 36: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 76ms/step - loss: 0.0037 - acc: 0.9969 - val_loss: 0.0105 - val_acc: 0.9862 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9965\n",
      "Epoch 37: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0045 - acc: 0.9966 - val_loss: 0.0191 - val_acc: 0.9707 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9954\n",
      "Epoch 38: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 74ms/step - loss: 0.0062 - acc: 0.9954 - val_loss: 0.0113 - val_acc: 0.9879 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 39: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 74ms/step - loss: 0.0024 - acc: 0.9987 - val_loss: 0.0088 - val_acc: 0.9862 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9937\n",
      "Epoch 40: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 74ms/step - loss: 0.0064 - acc: 0.9937 - val_loss: 0.0165 - val_acc: 0.9793 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9973\n",
      "Epoch 41: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0052 - acc: 0.9973 - val_loss: 0.0114 - val_acc: 0.9897 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9977\n",
      "Epoch 42: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0028 - acc: 0.9977 - val_loss: 0.0155 - val_acc: 0.9810 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9952\n",
      "Epoch 43: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 25s 75ms/step - loss: 0.0050 - acc: 0.9952 - val_loss: 0.0094 - val_acc: 0.9862 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9958\n",
      "Epoch 44: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 28s 85ms/step - loss: 0.0048 - acc: 0.9958 - val_loss: 0.0101 - val_acc: 0.9897 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9981\n",
      "Epoch 45: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 27s 82ms/step - loss: 0.0026 - acc: 0.9981 - val_loss: 0.0089 - val_acc: 0.9897 - lr: 3.1623e-04\n",
      "Epoch 46/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9992\n",
      "Epoch 46: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 74ms/step - loss: 0.0013 - acc: 0.9992 - val_loss: 0.0084 - val_acc: 0.9897 - lr: 3.1623e-04\n",
      "Epoch 47/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9992\n",
      "Epoch 47: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 0.0012 - acc: 0.9992 - val_loss: 0.0083 - val_acc: 0.9897 - lr: 3.1623e-04\n",
      "Epoch 48/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 9.7846e-04 - acc: 0.9992\n",
      "Epoch 48: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 9.7818e-04 - acc: 0.9992 - val_loss: 0.0078 - val_acc: 0.9897 - lr: 3.1623e-04\n",
      "Epoch 49/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9992\n",
      "Epoch 49: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 26s 81ms/step - loss: 0.0010 - acc: 0.9992 - val_loss: 0.0080 - val_acc: 0.9897 - lr: 3.1623e-04\n",
      "Epoch 50/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 7.8663e-04 - acc: 0.9996\n",
      "Epoch 50: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 28s 87ms/step - loss: 7.8620e-04 - acc: 0.9996 - val_loss: 0.0083 - val_acc: 0.9862 - lr: 3.1623e-04\n",
      "Epoch 51/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 6.4030e-04 - acc: 0.9996\n",
      "Epoch 51: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 24s 75ms/step - loss: 6.3993e-04 - acc: 0.9996 - val_loss: 0.0084 - val_acc: 0.9879 - lr: 3.1623e-04\n",
      "Epoch 52/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 6.6398e-04 - acc: 0.9996\n",
      "Epoch 52: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 27s 82ms/step - loss: 6.6360e-04 - acc: 0.9996 - val_loss: 0.0108 - val_acc: 0.9862 - lr: 3.1623e-04\n",
      "Epoch 53/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 6.5646e-04 - acc: 0.9996\n",
      "Epoch 53: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 28s 85ms/step - loss: 6.5608e-04 - acc: 0.9996 - val_loss: 0.0088 - val_acc: 0.9897 - lr: 3.1623e-04\n",
      "Epoch 54/200\n",
      "326/327 [============================>.] - ETA: 0s - loss: 6.5042e-04 - acc: 0.9996\n",
      "Epoch 54: val_acc did not improve from 0.98966\n",
      "327/327 [==============================] - 28s 84ms/step - loss: 6.5006e-04 - acc: 0.9996 - val_loss: 0.0081 - val_acc: 0.9879 - lr: 3.1623e-04\n",
      "Training time is： 1360.2837908267975 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    create = QBTransNet(input_shape=(20,6000,3), n_class=2, routings=3)\n",
    "    history = run_experiment(create)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate training time\n",
    "    training_time = end_time - start_time\n",
    "    print(\"Training time is：\", training_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87800808-3294-480e-8c9a-caca2a0918e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
